\documentclass[fleqn,10pt]{wlpeerj}
%% DE created stub file June 30, 2014 initially set up for PeerJ
%% Have at - change whatever you like.
%% Text is being tracked using Mercurial for revision control.  

% some packages here 
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage[hidelinks]{hyperref}
\usepackage{lineno} % for review only

% define some things here
\newcommand{\argus}{\texttt{argus}}
\newcommand{\detectpatterns}{\texttt{argus\_detect\_patterns}}
\newcommand{\simplified}{\texttt{argus\_simplified}}
\newcommand{\pipcommand}{\texttt{sudo pip install argus}}
\newcommand{\argusrepo}{\url{ssh://hg@bitbucket.org/devangel77b/argus}}
\newcommand{\matlabtoolbox}{add link here}

%\title{Fun things you can do with a bunch of GoPros} % interim
\title{3D to the People} % Ty


\author[1,2]{Brandon Jackson \thanks{author for correspondence: brandon.e.jackson@gmail.com}}
\author[2]{Dennis Evangelista}
\author[2]{Ty Hedrick}
\affil[1]{Longwood College, Charlottesville, VA}
\affil[2]{University of North Carolina at Chapel Hill, NC 27599-3280, USA}

\keywords{videography, photogrammetry, kinematics, multiple cameras, calibration}

\begin{abstract}
Ecological, behavioral, and biomechanical studies often need to quantify movement and behavior in three dimensions.  In laboratory studies, a major tool to accomplish these is the use of multiple, calibrated high-speed cameras.  Until very recently, complexity, weight and cost of such cameras has made their deployment in field situations risky; furthermore such cameras are often not affordable for early career researchers, teaching use by undergraduates, or for those not in biomechanics who don't have an overriding primary need for such toys.  Here we describe a solution and tool set using multiple inexpensive cameras to bridge such needs.  The availability of lower cost, portable and rugged solutions (and the likely future ubiquity of inexpensive cameras of reasonable performance) has promise to open up new areas of biological study by providing precise, 3D tracking and quantification of movement to more researchers. 
\end{abstract}

\begin{document}

% editing colors
\newcommand{\ty}[1]{{\color{blue} #1}}
\newcommand{\dennis}[1]{{\color{green} #1}}
\newcommand{\brandon}[1]{{\color{cyan} #1}}

\flushbottom
\maketitle
\thispagestyle{empty}

%% for line numbers
%\setpagewiselinenumbers
\modulolinenumbers[5]
\linenumbers

\section*{Introduction}

Introduction here. Citations here \citep{Bradski:2008}. 

Much of animal behavior, evolution, and ecology involves movement within complex three-dimensional (3D) environments.  Yet, few studies have tracked 3D movements of animals in natural settings, particularly animals in flight.  High-speed videography is a staple tool in studies of animal locomotion, and has led to foundational insight on mechanics of locomotion, the evolution of novel locomotor strategies, and recently the mechanics of non-steady and maneuvering performances.  

Laboratory-based studies of animal locomotion are necessarily limited in scope.  Most studies focus on single individuals of limited species performing standardized locomotor behaviors \ty{in a confined setting}.  The findings, while providing incredible insight to many aspects of animal locomotion, are therefore similarly limited in scope.  Some species are inherently more difficult than others to maintain in captivity, or to train to perform the requested task in a repeatable manner.  Some behaviors (e.g. predator-prey interactions, courtship, large group behaviors), on the other hand, are inherently \ty{difficult or }impossible to measure within the confines of a laboratory setting.  

A recently published \ty{protocol and associated software package} allows for researchers to overcome some of the previous hurdles to 3D videography in field settings \citep{Theriault:2014}.  Rather than using a carefully constructed calibration frame for calibration, the structure-from-motion technique depends on any objects in the field of view of multiple cameras, an object of known length (a “wand”), and an object \ty{or scene feature} providing orientation to global axes such as an object accelerating with gravity, a plumb line, or a horizontal plane.  \ty{The open source software implementations, provided in Python and MATLAB, represents a low-cost alternative to several commercially available packages.  However, the workflow described in Theriault et al. (\citeyear{Theriault:2014}) still assumes the use of costly laboratory grade cameras that provide hardware support for synchronizing shutter events among multiple cameras. These hardware requirements present an additional financial hurdle. Furthermore, laboratory grade cameras are rarely designed to be used in the field and are often senstive to dust, water and mechanical forces and have high electrical power requirements.}

Recent technological advancements, particularly related to expanding markets of extreme sports, have led to a vast expansion of high-quality consumer grade video and \ty{digital single-lens reflex (DSLR) cameras capable of moderately high-speed ($\le$250 frames per second) in color at resolutions comparable to or better than common laboratory cameras from 5 years ago. Furthermore, these consumer grade systems are much cheaper than their laboratory equivalents, are designed for stand-alone operation in outdoor field settings and are designed for continuous recording. However, they also lack hardware support for frame synchronization among many cameras - a key requirement for 3D videography according to Theriault et al. \cite{Theriault:2014} - and also often have wide angle, high distortion lenses that make camera calibration challenging.} The purpose of this work is to provide a set of instructions and software tools to aid researchers in integrating consumer-grade cameras into a relatively inexpensive and speedy workflow for 3D videography\ty{ in field or laboratory settings}.  

In this paper, we provide a simple work flow and tools aimed specifically at \ty{facilitating 3D videography} using multiple consumer-grade cameras (specifically, the GoPro Hero 3 series; although we have used the techniques here with Flip MinoHD and various models of GoPro Heros and DSLR cameras).  \ty{Our tools include a database of camera lens calibration parameters; simple tools for obtaining the lens parameters for cameras not in the database; methods synchronizing multiple cameras based on audio channel information; along with wand tracking (and keypoint detection?) to complete a 3D calibration.} Our goal in this paper is simplify the employment of 3D reconstruction techniques so that they may be used in ecological field studies, undergraduate teaching of biomechanics, \ty{laboratory research in settings without the funding support for pupose-built laboratory cameras} etc etc etc. 

\subsection*{How does 3D reconstruction work in a nutshell?}
Calibration of multiple cameras and using them to reconstruct position can be nontrival.  To use multiple cameras to reconstruct 3D positions requires knowledge of the cameras' intrinsic parameters, such as the lens focal length, principal point, and distortion coefficients.  It also requires knowledge of the extrinsic parameters: the relative positions and orientations of the cameras with respect to one another. Finally, 3D reconstruction generally requires knowing that the frames are synchronized in time.  While a number of tools exist to actually perform the calibration (Borguet, Theriault et al) they can still be daunting to general users.

Camera intrinsic parameters define...

Full 3D reconstruction also requires knowledge of the relative positions an orientations of multiple cameras..details on calculating camera intrinsics, wand calibrations, and the need for auto wand tracking

Lastly, 3D reconstruction works by using camera intrinsics and extrinics to triangulate an object's position in 3D space based on the images in each camera.  If the object of interest is moving, the images from each camera must be taken at the same time. While laboratory grade cameras include frame and exposure synchronization functions in hardware, consumer-grade cameras generally lack this ability in any form.  
\subsection*{Ecologist-proof roadmap of how to set up to do it}

\section*{Methods and materials}
\subsection*{Cameras}
We demonstrate these methods using GoPro Hero3 Black cameras (GoPro, Santa Cruz, CA, USA), mounted in stock cases (serial numbers), as well as Nikon D300S DSLR with 24 mm lenses. 

\subsection*{Software tools}
Several software tools exist for providing camera calibrations, including (citations).  Here we present a simplified set of tools optimized for ease of use: the Python \argus\ package (\argusrepo, or install using \pipcommand) and the Matlab (NAME) toolbox (\matlabtoolbox). % are we still doing parallel Python and MATLAB workflows?  Python is definitely further along at this time and will probably be more feature rich even if we build in parallel -Ty 

\subsection*{Laboratory calibration of camera intrinsics}
An online library of camera intrinsic parameters is provided (HERE).  The Python \argus\ package can be used to obtain a laboratory calibration for camera intrinsics, as a check or for a configuration not provided. First, a test pattern (LINK) is printed and firmly affixed to a flat surface; we typically use either a (dimensions) chessboard pattern or a (dimensions) dot pattern (see Figure X).  The camera is set for the desired resolution, frame rate, and field of view and the patterns are filmed; the use of an external monitor can aid the process.  

The resulting video is analyzed frame by frame to locate the patterns, using the \detectpatterns\ script.  The detected patterns are used to iteratively find a set of intrinsic parameters that minimizes the root mean squared error (rmse) of the projection; this is accomplished with the \simplified\ script.  We have configured the \simplified\ script to work for most typical applications; additional scripts are provided in \argus\ to provide fine control over the order of parameter optimization and the detailed stage-by-stage minimization search.

Move to results: As an example of a calibration, we filmed a (dimensions) pattern using a GoPro Hero3 Black camera (figure X).  The resulting calibration data is given in Table X. To examine the spread in calibration values, we repeated the calibration 2000 times using 30 patterns for each replicate. The best rmse value falls within the 25th-75th interquartile range and is near the median value for the best 200 calibrations, suggesting we are not in a local minimum and that the calibration parameters are significant...   

\begin{figure}
\caption{(a) Example video frame showing acquisition of GoPro Hero3 Black camera lens intrinsics from a dot grid.  (b) Pattern detected using the \detectpatterns\ Python script.}
\label{fig:labcal1}
\end{figure}

\begin{table}
\caption{Results of laboratory calibration of GoPro Hero3 Black camera intrinsics using 5000 replicates, showing best rmse and the median, 25th and 75th percentile values of the top 100 solutions? }
\label{tab:labcal2}
\begin{center}
\begin{tabular}{lcccc}
parameter & best & 25\% & median & 75\% \\
\cline{1-5}
focal length $f$, pixels & 1209 & & &\\
principal point $c_x$, pixels & 639.5 & & & \\
principal point $c_y$, pixels & 359.5 & & & \\
aspect ratio $AR$ & 1 & 1 & 1 & 1\\
skew $s$ & 0 & 0 & 0 & 0 \\
radial $k_1$ & -0.3427 & & & \\
radial $k_2$ & 0.1306 & & & \\
tangential $t_1$ & 0 & 0 & 0 & 0 \\
tangential $t_2$ & 0 & 0 & 0 & 0 \\
radial $k_3$ & 0 & & & \\
\end{tabular}
\end{center}
note: $c_x$, $c_y$, $AR$, $s$, $t_1$ and $t_2$, and $k_3$ held fixed during optimization.
\end{table}


\subsection*{Field deployment and field calibration of camera extrinsics}
\subsection*{3D reconstruction}

\section*{Results and discussion}
\subsection*{Simple lab example}
\subsection*{Simple field example}

\section*{Acknowledgements}
Thank you everyone.

\bibliography{gopro}
\end{document}
